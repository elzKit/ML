{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression\n",
    "\n",
    "## Basic idea\n",
    "We use linear regression as the baseline, but we map the results (from -infinity to +infinity) to the interval (0,1) and interpret the results as probability that the given data point belongs to a class.\n",
    "\n",
    "We call the function which does the mapping a \"link function\".\n",
    "\n",
    "For the logistic regression, as use the sigmoid function as link function:\n",
    "$$\n",
    "P(y_i = +1 | \\mathbf{x}_i,\\mathbf{w}) = \\frac{1}{1 + \\exp(-\\mathbf{w}^T h(\\mathbf{x}_i))}\n",
    "$$\n",
    "\n",
    "## Model\n",
    "Generalized linear model.\n",
    "\n",
    "\n",
    "## Learning\n",
    "\n",
    "We choose the coefficients to maximize the likelihood function.\n",
    "\n",
    "In order to simplify math, we use the log-likelihood:\n",
    "\n",
    "\n",
    "$$\\ell\\ell(\\mathbf{w}) = \\sum_{i=1}^N \\Big( (\\mathbf{1}[y_i = +1] - 1)\\mathbf{w}^T h(\\mathbf{x}_i) - \\ln\\left(1 + \\exp(-\\mathbf{w}^T h(\\mathbf{x}_i))\\right) \\Big) $$\n",
    "\n",
    "The log-likelihood derivative is:\n",
    "$$\n",
    "\\frac{\\partial\\ell}{\\partial w_j} = \\sum_{i=1}^N h_j(\\mathbf{x}_i)\\left(\\mathbf{1}[y_i = +1] - P(y_i = +1 | \\mathbf{x}_i, \\mathbf{w})\\right)\n",
    "$$\n",
    "\n",
    "\n",
    "## Regularization\n",
    "As usual, regularization allows us to avoid overfitting. \n",
    "Overfitting symptoms for logistic regression are:\n",
    "* large coefficient values\n",
    "* overconfident estimations (very high(small) probability values, close to 1(0), basically no incertainty zone\n",
    "* test set accuracy worse than training set accuracy\n",
    "\n",
    "### L2 Regularization\n",
    "\n",
    "\n",
    "\n",
    "* The per-coefficient derivative of log likelihood:\n",
    "$$\n",
    "\\frac{\\partial\\ell}{\\partial w_j} = \\sum_{i=1}^N h_j(\\mathbf{x}_i)\\left(\\mathbf{1}[y_i = +1] - P(y_i = +1 | \\mathbf{x}_i, \\mathbf{w})\\right) \\color{red}{-2\\lambda w_j }\n",
    "$$\n",
    "\n",
    "We do not regularize the intercept term, so that for $w_0$ we have:\n",
    "$$\n",
    "\\frac{\\partial\\ell}{\\partial w_0} = \\sum_{i=1}^N h_0(\\mathbf{x}_i)\\left(\\mathbf{1}[y_i = +1] - P(y_i = +1 | \\mathbf{x}_i, \\mathbf{w})\\right)\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "### L1 Regularization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
