{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression\n",
    "\n",
    "## Model\n",
    "We use linear regression as the baseline, but we map the results (from -infinity to +infinity) to the interval (0,1) and interpret the results as probability that the given data point belongs to a claLogistic regression has a linear score with a logistic link function.ss.\n",
    "\n",
    "We call the function which does the mapping a \"link function\".\n",
    "Logistic regression has a linear score with a logistic  (sigmoid) ( cumulative logistic distribution)link function.\n",
    "\n",
    "$$\n",
    "P(y_i = +1 | \\mathbf{x}_i,\\mathbf{w}) = \\frac{1}{1 + \\exp(-\\mathbf{w}^T h(\\mathbf{x}_i))}\n",
    "$$\n",
    "\n",
    "Logistic regression is a type of linear classifier, since the output is the weighted sum of inputs.\n",
    "For linear classifiers, the decision boundary is a line/plane/hyperplane.\n",
    "\n",
    "Logistic regression can be seen as a special case of generalized linear model and thus analogous to linear regression. \n",
    "\n",
    "\n",
    "## Learning\n",
    "\n",
    "We choose the coefficients to maximize the likelihood function. For the negative data points we want to maximize the probability that the output y is 0, and for the positive data points we want to maximize the probability that the output y is 1.\n",
    "The probabilities of the specific data points are combined with multiplication operation.\n",
    "$$\\ell(\\mathbf{w}) = \\prod_{i=1}^N P(y_i | \\mathbf{x}_i, \\mathbf{w})$$\n",
    "\n",
    "In order to simplify math, we use the log-likelihood:\n",
    "\n",
    "ln of the above\n",
    "\n",
    "$$\\ell\\ell(\\mathbf{w}) = \\sum_{i=1}^N \\Big( (\\mathbf{1}[y_i = +1] - 1)\\mathbf{w}^T h(\\mathbf{x}_i) - \\ln\\left(1 + \\exp(-\\mathbf{w}^T h(\\mathbf{x}_i))\\right) \\Big) $$\n",
    "\n",
    "In order to find the max, we use gradient ascent, which is based on calculating the derivative. (There is no close form solution).\n",
    "\n",
    "The log-likelihood derivative is:\n",
    "$$\n",
    "\\frac{\\partial\\ell}{\\partial w_j} = \\sum_{i=1}^N h_j(\\mathbf{x}_i)\\left(\\mathbf{1}[y_i = +1] - P(y_i = +1 | \\mathbf{x}_i, \\mathbf{w})\\right)\n",
    "$$\n",
    "\n",
    "Gradient ascent: the w(t+1) = w(t) + step_size * gradient of ll with respect to w\n",
    "\n",
    "\n",
    "###choosing the step size\n",
    "If too small - slow convergence\n",
    "if too big - divergence or oscillations\n",
    "\n",
    "To choose, try several values, exponentially spaced\n",
    "\n",
    "\n",
    "## Regularization\n",
    "As usual, regularization allows us to avoid overfitting. \n",
    "Overfitting symptoms for logistic regression are:\n",
    "* large coefficient values\n",
    "* overconfident estimations (very high(small) probability values, close to 1(0), basically no incertainty zone\n",
    "* test set accuracy worse than training set accuracy\n",
    "\n",
    "### L2 Regularization\n",
    "\n",
    "\n",
    "\n",
    "* The per-coefficient derivative of log likelihood:\n",
    "$$\n",
    "\\frac{\\partial\\ell}{\\partial w_j} = \\sum_{i=1}^N h_j(\\mathbf{x}_i)\\left(\\mathbf{1}[y_i = +1] - P(y_i = +1 | \\mathbf{x}_i, \\mathbf{w})\\right) \\color{red}{-2\\lambda w_j }\n",
    "$$\n",
    "\n",
    "We do not regularize the intercept term, so that for $w_0$ we have:\n",
    "$$\n",
    "\\frac{\\partial\\ell}{\\partial w_0} = \\sum_{i=1}^N h_0(\\mathbf{x}_i)\\left(\\mathbf{1}[y_i = +1] - P(y_i = +1 | \\mathbf{x}_i, \\mathbf{w})\\right)\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "### L1 Regularization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
